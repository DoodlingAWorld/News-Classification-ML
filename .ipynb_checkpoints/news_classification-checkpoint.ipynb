{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a58de300-bfe9-4b9c-93f3-06c33106ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy import tokenizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import LsiModel, TfidfModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba671ae7-f26d-485c-a1ab-391b816938fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "default_plot_colour = \"#00bfbf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "576583e5-52e1-4c42-a66c-4c2db35471a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>fake_or_factual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOLLYWEIRD LIB SUSAN SARANDON Compares Muslim ...</td>\n",
       "      <td>There are two small problems with your analogy...</td>\n",
       "      <td>Dec 30, 2015</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elijah Cummings Called Trump Out To His Face ...</td>\n",
       "      <td>Buried in Trump s bonkers interview with New Y...</td>\n",
       "      <td>April 6, 2017</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hillary Clinton Says Half Her Cabinet Will Be...</td>\n",
       "      <td>Women make up over 50 percent of this country,...</td>\n",
       "      <td>April 26, 2016</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russian bombing of U.S.-backed forces being di...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. Defense Secretary ...</td>\n",
       "      <td>September 18, 2017</td>\n",
       "      <td>Factual News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Britain says window to restore Northern Irelan...</td>\n",
       "      <td>BELFAST (Reuters) - Northern Ireland s politic...</td>\n",
       "      <td>September 4, 2017</td>\n",
       "      <td>Factual News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  HOLLYWEIRD LIB SUSAN SARANDON Compares Muslim ...   \n",
       "1   Elijah Cummings Called Trump Out To His Face ...   \n",
       "2   Hillary Clinton Says Half Her Cabinet Will Be...   \n",
       "3  Russian bombing of U.S.-backed forces being di...   \n",
       "4  Britain says window to restore Northern Irelan...   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  There are two small problems with your analogy...         Dec 30, 2015   \n",
       "1  Buried in Trump s bonkers interview with New Y...        April 6, 2017   \n",
       "2  Women make up over 50 percent of this country,...       April 26, 2016   \n",
       "3  WASHINGTON (Reuters) - U.S. Defense Secretary ...  September 18, 2017    \n",
       "4  BELFAST (Reuters) - Northern Ireland s politic...   September 4, 2017    \n",
       "\n",
       "  fake_or_factual  \n",
       "0       Fake News  \n",
       "1       Fake News  \n",
       "2       Fake News  \n",
       "3    Factual News  \n",
       "4    Factual News  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"fake_news_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4e870-3f59-452d-ba4a-dde702464ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62c03d-3531-4a60-bdf9-f40aa9b6fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of fake and factual articles\n",
    "data['fake_or_factual'].value_counts().plot(kind='bar', color=default_plot_colour)\n",
    "plt.title('Count of Article Classification')\n",
    "plt.ylabel('# of Articles')\n",
    "plt.xlabel('Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c9724-01a6-478a-bcef-bb1ca06c2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "fake_news = data[data['fake_or_factual'] == \"Fake News\"]\n",
    "fact_news = data[data['fake_or_factual'] == \"Factual News\"]\n",
    "\n",
    "# creating the spacy docs and using pipe for the dataframe\n",
    "fake_spaceydocs = list(nlp.pipe(fake_news['text']))\n",
    "fact_spaceydocs = list(nlp.pipe(fact_news['text'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6ced08-796c-41fd-9fe8-f351b16cb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to extract tags for each doc in the data\n",
    "def extract_tags(doc:spacy.tokens.doc.Doc):\n",
    "    return [(i.text, i.ent_type_, i.pos_) for i in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c5694-b3d4-411a-9e93-0f13798725b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag fake dataset \n",
    "fake_tagsdf = []\n",
    "columns = [\"token\", \"ner_tag\", \"pos_tag\"]\n",
    "\n",
    "for ix, doc in enumerate(fake_spaceydocs):\n",
    "    tags = extract_tags(doc)\n",
    "    tags = pd.DataFrame(tags)\n",
    "    tags.columns = columns\n",
    "    fake_tagsdf.append(tags)\n",
    "        \n",
    "fake_tagsdf = pd.concat(fake_tagsdf)   \n",
    "\n",
    "# tag factual dataset \n",
    "fact_tagsdf = []\n",
    "\n",
    "for ix, doc in enumerate(fact_spaceydocs):\n",
    "    tags = extract_tags(doc)\n",
    "    tags = pd.DataFrame(tags)\n",
    "    tags.columns = columns\n",
    "    fact_tagsdf.append(tags)\n",
    "        \n",
    "fact_tagsdf = pd.concat(fact_tagsdf)\n",
    "\n",
    "fake_tagsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e91a3-c7eb-44e5-bc7b-df8ab66a38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake token freq count\n",
    "pos_counts_fake = fake_tagsdf.groupby(['token','pos_tag']).size().reset_index(name='counts').sort_values(by='counts', ascending=False)\n",
    "pos_counts_fake.head(10)\n",
    "\n",
    "# fact token freq count\n",
    "pos_counts_fact = fact_tagsdf.groupby(['token','pos_tag']).size().reset_index(name='counts').sort_values(by='counts', ascending=False)\n",
    "pos_counts_fact.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8153f7b-c6c5-471a-b052-c7afbfa9d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies of pos tags\n",
    "pos_counts_fake.groupby(['pos_tag'])['token'].count().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79408e85-9bb2-4cef-a5f3-cfbdb4714606",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fact.groupby(['pos_tag'])['token'].count().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de4056-8b80-4bc9-bda6-a05eee5d89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going through the differences in nouns\n",
    "pos_counts_fake[pos_counts_fake.pos_tag == \"NOUN\"][0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd681649-feb4-4fd3-987c-eb97b8ae666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts_fact[pos_counts_fact.pos_tag == \"NOUN\"][0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0766525-9371-4441-8cb0-0f87d0bcb490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking top entities in fake news\n",
    "top_entities_fake = fake_tagsdf[fake_tagsdf['ner_tag'] != \"\"] \\\n",
    "                    .groupby(['token','ner_tag']).size().reset_index(name='counts') \\\n",
    "                    .sort_values(by='counts', ascending=False)\n",
    "\n",
    "# Ranking top entities in fact news\n",
    "top_entities_fact = fact_tagsdf[fact_tagsdf['ner_tag'] != \"\"] \\\n",
    "                    .groupby(['token','ner_tag']).size().reset_index(name='counts') \\\n",
    "                    .sort_values(by='counts', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de885471-e1bc-4c0c-a986-cc75338d974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just some colours to give consistent plots\n",
    "ner_palette = {\n",
    "    'ORG': sns.color_palette(\"Set2\").as_hex()[0],\n",
    "    'GPE': sns.color_palette(\"Set2\").as_hex()[1],\n",
    "    'NORP': sns.color_palette(\"Set2\").as_hex()[2],\n",
    "    'PERSON': sns.color_palette(\"Set2\").as_hex()[3],\n",
    "    'DATE': sns.color_palette(\"Set2\").as_hex()[4],\n",
    "    'CARDINAL': sns.color_palette(\"Set2\").as_hex()[5],\n",
    "    'PERCENT': sns.color_palette(\"Set2\").as_hex()[6]\n",
    "}\n",
    "sns.barplot(\n",
    "    x = 'counts',\n",
    "    y = 'token',\n",
    "    hue = 'ner_tag',\n",
    "    palette = ner_palette,\n",
    "    data = top_entities_fake[0:10],\n",
    "    orient = 'h',\n",
    "    dodge=False\n",
    ") \\\n",
    ".set(title='Most Common Entities in Fake News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b392a7-9975-407a-b52d-7fff78f9bc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    x = 'counts',\n",
    "    y = 'token',\n",
    "    hue = 'ner_tag',\n",
    "    palette = ner_palette,\n",
    "    data = top_entities_fact[0:10],\n",
    "    orient = 'h',\n",
    "    dodge=False\n",
    ") \\\n",
    ".set(title='Most Common Entities in Factual News')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8901b-de91-4fd2-98e7-0f2b24343c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing\n",
    "# removing location tags at the beginning of the article using regex\n",
    "data['text_clean'] = data.apply(lambda x: re.sub(r\"^[^-]*-\\s*\", \"\", x['text']), axis=1)\n",
    "# lowercase\n",
    "data['text_clean'] = data['text_clean'].str.lower()\n",
    "# and removing punctuation + stop words\n",
    "data['text_clean'] = data.apply(lambda x: re.sub(r\"([^\\w\\s])\", \"\", x['text_clean']), axis=1)\n",
    "en_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08912da6-43dc-4125-a3db-93733bcf9588",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e2de6-a978-4ee9-9b44-ae7ad5c28c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_clean'] = data['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (en_stopwords)]))\n",
    "\n",
    "# Now tokenizing + lemmatizing\n",
    "data['text_clean'] = data.apply(lambda x: word_tokenize(x['text_clean']), axis=1)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data[\"text_clean\"] = data[\"text_clean\"].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a2a96-79b0-40cd-b528-4e61fa512d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efd6f4-5cd6-4ce1-acea-6b3e43334b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common unigrams after preprocessing\n",
    "tokens_clean = sum(data['text_clean'], [])\n",
    "unigrams = (pd.Series(nltk.ngrams(tokens_clean, 1)).value_counts()).reset_index()[:10]\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f32a1-35b5-4b3b-b32c-095999f4f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams['token'] = unigrams['index'].apply(lambda x: x[0]) # extract the token from the tuple so we can plot it\n",
    "\n",
    "sns.barplot(x = \"count\", \n",
    "            y = \"token\", \n",
    "            data=unigrams,\n",
    "            orient = 'h',\n",
    "            palette=[default_plot_colour],\n",
    "            hue = \"token\", legend = False)\\\n",
    ".set(title='Most Common Unigrams After Preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e03c2-bdca-436b-ad2e-e3220731e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now most common bigrams are preprocessing\n",
    "bigrams = (pd.Series(nltk.ngrams(tokens_clean, 2)).value_counts()) \n",
    "print(bigrams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be60b7c-5c54-4729-806c-9fe6317df877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "vader_sentiment = SentimentIntensityAnalyzer()\n",
    "data['vader_sentiment_score'] = data['text'].apply(lambda review: vader_sentiment.polarity_scores(review)['compound'])\n",
    "\n",
    "# creating labels\n",
    "bins = [-1, -0.1, 0.1, 1]\n",
    "names = ['negative', 'neutral', 'positive']\n",
    "\n",
    "data['vader_sentiment_label'] = pd.cut(data['vader_sentiment_score'], bins, labels=names)\n",
    "data['vader_sentiment_label'].value_counts().plot.bar(color=default_plot_colour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa7c1d-fabc-4757-aaef-c0d44f41dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(\n",
    "    x = 'fake_or_factual',\n",
    "    hue = 'vader_sentiment_label',\n",
    "    palette = sns.color_palette(\"hls\"),\n",
    "    data = data\n",
    ") \\\n",
    ".set(title='Sentiment by News Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04238023-a517-448c-a8cb-ad2507b4f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA stuff now\n",
    "# vectorizing fake news\n",
    "fake_news_text = data[data['fake_or_factual'] == \"Fake News\"]['text_clean'].reset_index(drop=True)\n",
    "dictionary_fake = corpora.Dictionary(fake_news_text)\n",
    "doc_term_fake = [dictionary_fake.doc2bow(text) for text in fake_news_text]\n",
    "\n",
    "# generate coherence scores to determine an optimum number of topics\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "\n",
    "for num_topics_i in range(min_topics, max_topics+1):\n",
    "    model = gensim.models.LdaModel(doc_term_fake, num_topics=num_topics_i, id2word = dictionary_fake)\n",
    "    model_list.append(model)\n",
    "    coherence_model = CoherenceModel(model=model, texts=fake_news_text, dictionary=dictionary_fake, coherence='c_v')\n",
    "    coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "plt.plot(range(min_topics, max_topics+1), coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427eb611-34e3-4f7d-a8f0-98bb59071939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lda model\n",
    "num_topics_fake = 6 \n",
    "\n",
    "lda_model_fake = gensim.models.LdaModel(corpus=doc_term_fake,\n",
    "                                       id2word=dictionary_fake,\n",
    "                                       num_topics=num_topics_fake)\n",
    "\n",
    "lda_model_fake.print_topics(num_topics=num_topics_fake, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6324e50a-2297-4b6f-a83e-cc79db029011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing LSA analysis TF-IDF vectorization\n",
    "\n",
    "# lot of common if not very similar words\n",
    "def tfidf_corpus(doc_term_matrix):\n",
    "    # create a corpus using tfidf vecotization\n",
    "    tfidf = TfidfModel(corpus=doc_term_matrix, normalize=True)\n",
    "    corpus_tfidf = tfidf[doc_term_matrix]\n",
    "    return corpus_tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba9666-c780-445f-aa24-c1a034305d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coherence_scores(corpus, dictionary, text, min_topics, max_topics):\n",
    "    # generate coherence scores to determine an optimum number of topics\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics_i in range(min_topics, max_topics+1):\n",
    "        model = LsiModel(corpus, num_topics=num_topics_i, id2word = dictionary, random_seed=0)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=text, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    # plot results\n",
    "    plt.plot(range(min_topics, max_topics+1), coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21deaf6-07a9-458f-a671-5bb0847e32cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tfidf representation\n",
    "corpus_tfidf_fake = tfidf_corpus(doc_term_fake)\n",
    "# coherence scores for fake news data\n",
    "get_coherence_scores(corpus_tfidf_fake, dictionary_fake, fake_news_text, min_topics=2, max_topics=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd931bd6-5863-4fa4-97c2-a800e0e69269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake news model data\n",
    "lsa_fake = LsiModel(corpus_tfidf_fake, id2word=dictionary_fake, num_topics=5)\n",
    "lsa_fake.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef537b45-1096-440e-97c8-aad4b68f81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting whether fake or factual news\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1682e3-861e-476d-a730-f87af75c9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [','.join(map(str, l)) for l in data['text_clean']]\n",
    "Y = data['fake_or_factual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69861ae-bf8e-4fb3-a1f2-c6b76cd0790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer - text vectorization\n",
    "countvec = CountVectorizer()\n",
    "countvec_fit = countvec.fit_transform(X)\n",
    "bag_of_words = pd.DataFrame(countvec_fit.toarray(), columns = countvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688768c8-7ac1-482d-a625-1fb8232c917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac65d1-5c66-464e-8861-dbfb86cd0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ead4fa-aa20-4daf-88b2-949b6603a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afadd0df-2898-4b70-9685-d63ba830585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_lr, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08492d-705b-4e66-921c-13deffd5b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9ac0b-79ce-4fa8-8bf5-03cfea3e4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SGDClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdb78b1-af54-473a-8afa-489fa496a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25e736-896f-4de3-aed4-9e07417e988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred_svm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9e397-313d-4d0c-9f32-f2a94e7f014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f434f-e06d-4d49-9f01-946f109b2545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course_env",
   "language": "python",
   "name": "nlp_course_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
